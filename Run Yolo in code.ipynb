{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe81cff0-a624-4526-bf8f-7488eb9e8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import check_img_size\n",
    "from utils.torch_utils import select_device, TracedModel\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.plots import plot_one_box, plot_one_box_PIL\n",
    "from copy import deepcopy\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, atan, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5f299c-5d94-4aee-b89f-0f6365578e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahab\\qenv\\lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\\nplate_image = cv.imread(image_path)\\n# detected_plate_image = get_plates_from_image(plate_image)\\n# cv.imwrite(os.path.join(savepath, \"detected_plate.png\"), detected_plate_image)\\n# cv.imshow(\"detected_plate_image\",detected_plate_image)\\n# cv.waitKey(0)\\n# cv.destroyAllWindows\\n\\n#detected_plate_image = get_plates_from_video(video_path)\\n\\n#detected_plate_webcam = get_plates_from_webcam()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_n_vids_path = \"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Car/data-for-yolov7/test/images\"\n",
    "image_path = os.path.join(images_n_vids_path, \"img1.jpg\")\n",
    "#video_path = os.path.join(images_n_vids_path, \"test_video_short.mp4\")\n",
    "\n",
    "savepath = \"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Result-Yolov7\"\n",
    "weights = 'weights/best.pt'\n",
    "device_id = 'cpu'\n",
    "image_size = 640\n",
    "trace = True\n",
    "\n",
    "# Initialize\n",
    "device = select_device(device_id)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "stride = int(model.stride.max())  # model stride\n",
    "imgsz = check_img_size(image_size, s=stride)  # check img_size\n",
    "\n",
    "if trace:\n",
    "    model = TracedModel(model, device, image_size)\n",
    "\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "    \n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "\n",
    "# Load OCR\n",
    "reader = easyocr.Reader(['fa'])\n",
    "\n",
    "\n",
    "def detect_plate(source_image):\n",
    "    # Padded resize\n",
    "    img_size = 640\n",
    "    stride = 32\n",
    "    img = letterbox(source_image, img_size, stride=stride)[0]\n",
    "\n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # Inference\n",
    "        pred = model(img, augment=True)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45, classes=0, agnostic=True)\n",
    "\n",
    "    plate_detections = []\n",
    "    det_confidences = []\n",
    "    \n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], source_image.shape).round()\n",
    "\n",
    "            # Return results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                coords = [int(position) for position in (torch.tensor(xyxy).view(1, 4)).tolist()[0]]\n",
    "                plate_detections.append(coords)\n",
    "                det_confidences.append(conf.item())\n",
    "\n",
    "    return plate_detections, det_confidences\n",
    "\n",
    "def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=2.0, threshold=0):\n",
    "    blurred = cv.GaussianBlur(image, kernel_size, sigma)\n",
    "    sharpened = float(amount + 1) * image - float(amount) * blurred\n",
    "    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n",
    "    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n",
    "    sharpened = sharpened.round().astype(np.uint8)\n",
    "    if threshold > 0:\n",
    "        low_contrast_mask = np.absolute(image - blurred) < threshold\n",
    "        np.copyto(sharpened, image, where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "def crop(image, coord):\n",
    "    cropped_image = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def ocr_plate(plate_region):\n",
    "    # Image pre-processing for more accurate OCR\n",
    "    #cv.imwrite(os.path.join(savepath, \"plate_img.png\"), plate_region)\n",
    "    rescaled = cv.resize(plate_region, None, fx=1.2, fy=1.2, interpolation=cv.INTER_CUBIC)\n",
    "    grayscale = cv.cvtColor(rescaled, cv.COLOR_BGR2GRAY)\n",
    "    # OCR the preprocessed image\n",
    "    grayscale_blur = cv.medianBlur(grayscale, 1)\n",
    "    ret, thresh1 = cv.threshold(grayscale_blur, 120, 255, cv.THRESH_BINARY + cv.THRESH_OTSU) \n",
    "    #cv.imwrite(os.path.join(savepath, \"grayscale_blur.png\"), grayscale_blur)\n",
    "    plate_text_easyocr = reader.readtext(grayscale_blur)\n",
    "    if plate_text_easyocr:\n",
    "        (bbox, text_easyocr, ocr_confidence) = plate_text_easyocr[0]\n",
    "        #print(\"plate_text Easyocr \", text_easyocr)\n",
    "    else:\n",
    "        text_easyocr = \"_\"\n",
    "        ocr_confidence = 0\n",
    "    #if ocr_confidence == 'nan':\n",
    "    \n",
    "    return text_easyocr, ocr_confidence\n",
    "\n",
    "#Rotate Pelak\n",
    "def find_longest_line(plate_img_gr):\n",
    "    kernel_size = 3\n",
    "    blur_gray = cv.GaussianBlur(plate_img_gr, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    low_threshold = 150\n",
    "    high_threshold = 200\n",
    "\n",
    "    edges = cv.Canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "    rho = 1  # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "    threshold = 15  # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 50  # minimum number of pixels making up a line\n",
    "    max_line_gap = 5  # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(plate_img_gr) * 0  # creating a blank to draw lines on\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv.HoughLinesP(edges, rho, theta, threshold, np.array([]),\n",
    "                        min_line_length, max_line_gap)\n",
    "\n",
    "    lls = []\n",
    "    for indx, line in enumerate(lines):\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv.line(line_image,(x1,y1),(x2,y2),(255,0,0),5)\n",
    "            line_length = sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "            lls.append((indx,line_length))\n",
    "    lls.sort(key = lambda x: x[1])\n",
    "    linessorted = []\n",
    "    for (indx,ll) in lls:\n",
    "        linessorted.append(lines[indx])\n",
    "    return linessorted\n",
    "\n",
    "def find_line_angle(line):\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    angle = degrees(atan(((y2-y1)/(x2-x1))))\n",
    "    return angle\n",
    "\n",
    "def rotate_image(plate_img_gr, angle):\n",
    "    (h, w) = plate_img_gr.shape\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "    M = cv.getRotationMatrix2D((cX, cY), angle, 1.0)\n",
    "    rotated = cv.warpAffine(plate_img_gr, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "def adjust_cropping(rotated_img):\n",
    "    h,w = rotated_img.shape\n",
    "    targ_h = int(w/4)\n",
    "    crop_h = int((h - targ_h)/2)\n",
    "    cropped_rotated_img = rotated_img[crop_h:h-crop_h,:]\n",
    "    return cropped_rotated_img\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "c = 0\n",
    "def Cutting_Pieces (plate_region):\n",
    "    #plate_region = cv.cvtColor(plate_region, cv.COLOR_BGR2GRAY)\n",
    "    #print(type(plate_region))\n",
    "    #print(plate_region.shape)\n",
    "    h, w = plate_region.shape\n",
    "    chopfactors = [(60,125), (125,190), (180,280), (280,350), (350,410), (410,470), (470,535), (535,600)]\n",
    "    plate_letters = []\n",
    "    global c\n",
    "    for factor in chopfactors:\n",
    "        w1 = int((factor[0]/600)*w)\n",
    "        w2 = int((factor[1]/600)*w)\n",
    "        letterpatch = plate_region[:,w1:w2]\n",
    "        letterpatch_resized  = cv.resize(letterpatch, (28,28), interpolation = cv.INTER_LINEAR)\n",
    "        #ax[c].imshow(letterpatch_resized, cmap='gray')\n",
    "        cv.imwrite(os.path.join(savepath, \"pieces/%s.png\" %c), letterpatch_resized)\n",
    "        c += 1\n",
    "        plate_letters.append(letterpatch_resized)\n",
    "\n",
    "ro = 0\n",
    "def get_plates_from_image(input):\n",
    "    if input is None:\n",
    "        return None\n",
    "    global ro\n",
    "    ro += 1\n",
    "    plate_detections, det_confidences = detect_plate(input)\n",
    "    plate_texts = []\n",
    "    ocr_confidences = []\n",
    "    detected_image = deepcopy(input)\n",
    "    for coords in plate_detections:\n",
    "        plate_region = crop(input, coords)\n",
    "        plate_text, ocr_confidence = ocr_plate(plate_region)\n",
    "        #Rotate Pelak\n",
    "        plate_img = plate_region.copy()\n",
    "        plate_img_gr = cv.cvtColor(plate_region, cv.COLOR_BGR2GRAY)\n",
    "        linessorted = find_longest_line(plate_img_gr)\n",
    "        rot_angle = find_line_angle(linessorted[-1])\n",
    "        rotated_img = rotate_image(plate_img_gr, rot_angle)\n",
    "        cropped_rotated_img = adjust_cropping(rotated_img)\n",
    "        cv.imwrite(os.path.join(savepath, \"rotate%s.png\" %ro), cropped_rotated_img)\n",
    "        #-------------------------------------------------------------------------------------\n",
    "        Cutting_Pieces(cropped_rotated_img)\n",
    "        plate_texts.append(plate_text)\n",
    "        ocr_confidences.append(ocr_confidence)\n",
    "        detected_image = plot_one_box_PIL(coords, detected_image, label=plate_text, color=[0, 150, 255], line_thickness=2)\n",
    "    return detected_image\n",
    "\n",
    "def pascal_voc_to_coco(x1y1x2y2):\n",
    "    x1, y1, x2, y2 = x1y1x2y2\n",
    "    return [x1, y1, x2 - x1, y2 - y1]\n",
    "\n",
    "def get_best_ocr(preds, rec_conf, ocr_res, track_id):\n",
    "    for info in preds:\n",
    "    # Check if it is current track id\n",
    "        if info['track_id'] == track_id:\n",
    "          # Check if the ocr confidenence is maximum or not\n",
    "            if info['ocr_conf'] < rec_conf:\n",
    "                info['ocr_conf'] = rec_conf\n",
    "                info['ocr_txt'] = ocr_res\n",
    "            else:\n",
    "                rec_conf = info['ocr_conf']\n",
    "                ocr_res = info['ocr_txt']\n",
    "            break\n",
    "    return preds, rec_conf, ocr_res\n",
    "\n",
    "def get_plates_from_video(source):\n",
    "    if source is None:\n",
    "        return None\n",
    "    \n",
    "    # Create a VideoCapture object\n",
    "    video = cv.VideoCapture(source)\n",
    "\n",
    "    # Default resolutions of the frame are obtained. The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    width = int(video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object.\n",
    "    temp = f'{Path(source).stem}_temp{Path(source).suffix}'\n",
    "    export = cv.VideoWriter(temp, cv.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Intializing tracker\n",
    "    tracker = DeepSort(embedder_gpu=False)\n",
    "    \n",
    "    # Initializing some helper variables.\n",
    "    preds = []\n",
    "    total_obj = 0\n",
    "\n",
    "    while(True):\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            # Run the ANPR algorithm\n",
    "            bboxes, scores = detect_plate(frame)\n",
    "            # Convert Pascal VOC detections to COCO\n",
    "            bboxes = list(map(lambda bbox: pascal_voc_to_coco(bbox), bboxes))\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Storing all the required info in a list.\n",
    "                detections = [(bbox, score, 'number_plate') for bbox, score in zip(bboxes, scores)]\n",
    "\n",
    "                # Applying tracker.\n",
    "                # The tracker code flow: kalman filter -> target association(using hungarian algorithm) and appearance descriptor.\n",
    "                tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "                # Checking if tracks exist.\n",
    "                for track in tracks:\n",
    "                    if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                        continue\n",
    "\n",
    "                    # Changing track bbox to top left, bottom right coordinates\n",
    "                    bbox = [int(position) for position in list(track.to_tlbr())]\n",
    "                    \n",
    "                    for i in range(len(bbox)):\n",
    "                        if bbox[i] < 0:\n",
    "                            bbox[i] = 0\n",
    "\n",
    "                    # Cropping the license plate and applying the OCR.\n",
    "                    plate_region = crop(frame, bbox)\n",
    "                    plate_text, ocr_confidence = ocr_plate(plate_region)\n",
    "\n",
    "                    # Storing the ocr output for corresponding track id.\n",
    "                    output_frame = {'track_id': track.track_id, 'ocr_txt': plate_text, 'ocr_conf': ocr_confidence}\n",
    "\n",
    "                    # Appending track_id to list only if it does not exist in the list\n",
    "                    # else looking for the current track in the list and updating the highest confidence of it.\n",
    "                    if track.track_id not in list(set(pred['track_id'] for pred in preds)):\n",
    "                        total_obj += 1\n",
    "                        preds.append(output_frame)\n",
    "                    else:\n",
    "                        preds, ocr_confidence, plate_text = get_best_ocr(preds, ocr_confidence, plate_text, track.track_id)\n",
    "                    \n",
    "                    # Plotting the prediction.\n",
    "                    frame = plot_one_box_PIL(bbox, frame, label=f'{str(track.track_id)}. {plate_text}', color=[255, 150, 0], line_thickness=3)\n",
    "                    cv.imshow(\"frame \", frame)\n",
    "                    keyexit = cv.waitKey(0)\n",
    "                    if keyexit == 27:\n",
    "                        break\n",
    "            # Write the frame into the output file\n",
    "            export.write(frame)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cv.destroyAllWindows()\n",
    "    video.release()\n",
    "    export.release()\n",
    "\n",
    "    # Compressing the output video for smaller size and web compatibility.\n",
    "    output = f'{Path(source).stem}_detected{Path(source).suffix}'\n",
    "    os.system(f'ffmpeg -y -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 1 -c:a aac -f mp4 /dev/null && ffmpeg -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 2 -c:a aac -movflags faststart {output}')\n",
    "    os.system(f'rm -rf {temp} ffmpeg2pass-0.log ffmpeg2pass-0.log.mbtree')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_plates_from_webcam():\n",
    "    \n",
    "    # Create a VideoCapture object\n",
    "    video = cv.VideoCapture(0)\n",
    "\n",
    "    # Default resolutions of the frame are obtained. The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    width = int(video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object.\n",
    "    temp = f'cam_temp.mp4'\n",
    "    export = cv.VideoWriter(temp, cv.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Intializing tracker\n",
    "    tracker = DeepSort(embedder_gpu=False)\n",
    "    \n",
    "    # Initializing some helper variables.\n",
    "    preds = []\n",
    "    total_obj = 0\n",
    "    fr_count = 0\n",
    "    while(True):\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            \n",
    "            fr_count+=1\n",
    "            if fr_count % 10 !=0:\n",
    "                continue\n",
    "\n",
    "            # Run the ANPR algorithm\n",
    "            bboxes, scores = detect_plate(frame)\n",
    "            # Convert Pascal VOC detections to COCO\n",
    "            bboxes = list(map(lambda bbox: pascal_voc_to_coco(bbox), bboxes))\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Storing all the required info in a list.\n",
    "                detections = [(bbox, score, 'number_plate') for bbox, score in zip(bboxes, scores)]\n",
    "\n",
    "                # Applying tracker.\n",
    "                # The tracker code flow: kalman filter -> target association(using hungarian algorithm) and appearance descriptor.\n",
    "                tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "                # Checking if tracks exist.\n",
    "                for track in tracks:\n",
    "                    if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                        continue\n",
    "\n",
    "                    # Changing track bbox to top left, bottom right coordinates\n",
    "                    bbox = [int(position) for position in list(track.to_tlbr())]\n",
    "                    \n",
    "                    for i in range(len(bbox)):\n",
    "                        if bbox[i] < 0:\n",
    "                            bbox[i] = 0\n",
    "\n",
    "                    # Cropping the license plate and applying the OCR.\n",
    "                    plate_region = crop(frame, bbox)\n",
    "                    plate_text, ocr_confidence = ocr_plate(plate_region)\n",
    "\n",
    "                    # Storing the ocr output for corresponding track id.\n",
    "                    output_frame = {'track_id': track.track_id, 'ocr_txt': plate_text, 'ocr_conf': ocr_confidence}\n",
    "\n",
    "                    # Appending track_id to list only if it does not exist in the list\n",
    "                    # else looking for the current track in the list and updating the highest confidence of it.\n",
    "                    if track.track_id not in list(set(pred['track_id'] for pred in preds)):\n",
    "                        total_obj += 1\n",
    "                        preds.append(output_frame)\n",
    "                    else:\n",
    "                        preds, ocr_confidence, plate_text = get_best_ocr(preds, ocr_confidence, plate_text, track.track_id)\n",
    "                    \n",
    "                    # Plotting the prediction.\n",
    "                    frame = plot_one_box_PIL(bbox, frame, label=f'{str(track.track_id)}. {plate_text}', color=[255, 150, 0], line_thickness=3)\n",
    "                    cv.imshow(\"frame \", frame)\n",
    "                    keyexit = cv.waitKey(0) \n",
    "                    if keyexit == 27:\n",
    "                        break\n",
    "            # Write the frame into the output file\n",
    "            export.write(frame)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cv.destroyAllWindows()\n",
    "    video.release()\n",
    "    export.release()\n",
    "\n",
    "    # Compressing the output video for smaller size and web compatibility.\n",
    "    output = f'cam_detected.mp4'\n",
    "    os.system(f'ffmpeg -y -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 1 -c:a aac -f mp4 /dev/null && ffmpeg -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 2 -c:a aac -movflags faststart {output}')\n",
    "    os.system(f'rm -rf {temp} ffmpeg2pass-0.log ffmpeg2pass-0.log.mbtree')\n",
    "\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "pip install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "plate_image = cv.imread(image_path)\n",
    "# detected_plate_image = get_plates_from_image(plate_image)\n",
    "# cv.imwrite(os.path.join(savepath, \"detected_plate.png\"), detected_plate_image)\n",
    "# cv.imshow(\"detected_plate_image\",detected_plate_image)\n",
    "# cv.waitKey(0)\n",
    "# cv.destroyAllWindows\n",
    "\n",
    "#detected_plate_image = get_plates_from_video(video_path)\n",
    "\n",
    "#detected_plate_webcam = get_plates_from_webcam()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e524992-a162-4044-ab17-6792e1dcfc66",
   "metadata": {},
   "source": [
    "# perform this plaque detection for a large collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a87f0c5-5280-47a8-8c4f-1e752b05f662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "imagefiles = glob.glob(\"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Car/data-for-yolov7/train/images/*.jpg\")\n",
    "imagefiles.sort()\n",
    "\n",
    "data = []\n",
    "for filename in imagefiles:\n",
    "    img = cv.imread(filename)\n",
    "    data.append(img)\n",
    "\n",
    "num_images = len(data)\n",
    "num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d24feb-f9b3-4391-b581-381ecfedc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahab\\AppData\\Local\\Temp\\ipykernel_12412\\2600293580.py:145: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  angle = degrees(atan(((y2-y1)/(x2-x1))))\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for image in data:\n",
    "    #plate_image = cv.imread(image_path)\n",
    "    try:\n",
    "        detected_plate_image = get_plates_from_image(image)\n",
    "        cv.imwrite(os.path.join(savepath, \"detected_plate%s.png\" %counter), detected_plate_image)\n",
    "        counter += 1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac65b8-031e-4cf2-a521-c664e41130c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642c513-d5c7-4817-93ec-47b8529601bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plate_image = cv.imread(image_path)\n",
    "#detected_plate_image = get_plates_from_image(plate_image)\n",
    "#cv.imwrite(os.path.join(savepath, \"detected_plate.png\"), detected_plate_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7dc36-4193-4cd3-a694-3d51caf71925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.cvShowImage(\"detected_plate_image\",detected_plate_image)\n",
    "#cv.waitKey(0)\n",
    "#cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a2a75-b8ab-4c68-b080-c80beff5a381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
